
c2 supervisev learning

- supervised machine learning
- mechine learning
whenever - cuando
-training set
but afterward automates - pero luego se automatiza
otherwise - de otra manera
infeasible task - tarea no factible

>> classification and regression

there are two major types - hay dos tipos principales
-classification and regression
- in classification, the goal is to predict a class label
benefit - beneficio
is in from the text - está dentro del texto

-for regression tasks, the goal is to predict a continuous number
annual income - ingresos anuales
a corn farm - una granja de maíz
weather - clima
even though - aunque
there is no matter of degree - no hay asunto de grado

>> generalization, overfitting, and uderfitting

as accurately as - con tanta precisión como
enough - suficiente
this can go wrong - esto puede salir mal
if we allow ourselves -si nos permitimos
we can always be as accurate - siempre podemos ser tan precisos
say a novice data scientist wants to predict boat - decir un científico novato de datos quiere predecir barco
a while -Un rato
but not bother those customers who won't be interested - Pero no molestes a los clientes que no estarán interesados.
come up - sube
boat - bote
of his does - de su hace
come up
age - años
twice - dos veces
customers - clientes
achieving - logrando
we could come up with that would explain - podríamos llegar a eso explicaría
the rule hinges - la regla depende de
+
we would trust - confiaríamos
- overfitting

too closely - demasiado cerca
everybody who owns - todo el mundo que posee
and your model will do badly even on the training set - y a tu modelo le irá mal incluso en el set de entrenamiento.
- uderfitting

there is a sweet spot in between - hay un punto dulce en el medio

sweet spot - punto justo

>> relation of model complexity to dataset size

is intimately tied - está íntimamente ligado
you can use without overfitting - Se puede usar sin sobre ajuste.
so larger - tan grande
+
can often work wonders - a menudo puede hacer maravillas

>> supervised machine learning algorithms

- model complexity
out for - Fuera por
the strengths and weaknesses - las fortalezas y debilidades
a better feeling - un sentimiento mejor

>> some sample datasets

highlight - destacar
- forge dataset
codigo 32
- wave dataset
codigo 33
migth not hold - podría no sostener
as long as - Mientras
+
of breast - de pecho
harmless

>> k-nearst neighbors

is arguably

>> k-neighbors classification

>> analyzing kneighborsclassifier

- decision boundary
breast
are rarely very smooth
fewer
flipped
drops
is lower
is even worse
is somewhere in the middle

>> k-neighbors regression

the single nearest neighbor
- KNeighbors Regressor class 
- score method
is a measure of goodness

>> analyzing kneighborsregressor

the predictions look like
a very unsteady prediction

>> strengths, weaknesses, and parameters

you should cetainly adjust this parameter

is somewhat
the strengths
without a lot adjustments
hundreads
badly
has neither

>> linear models

the last few decades
with roots going back over a hundred years
- linear function

>> linear models for regression

seems very restrictive
lost
a somewhat skewed perspective
can be very powwerful

>> linear regression (aka ordinary least squares)

-mean squared error
a benefit
but it also has no way
- coefficients
are likely underfitting
restricted
is a higher chance of overfitting
is much worse
- ridge regression
we will look into next

>> ridge regression

though
are chosen not only so that they predict
an additional constraint
to be as small as possible
while still predicting well
-regularization
so we are less likely to overfit
worse performance
trade-off
how much importance
through
to be less restricted
are barely restricted at all
and we end up 
we can also get a more qualitative insight
a more restricted model
the main takeway here
are mostly
are somewhat larger
are so large
54


